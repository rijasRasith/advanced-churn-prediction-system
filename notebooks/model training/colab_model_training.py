# -*- coding: utf-8 -*-
"""colab_model_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zZGb1MJ1oGi7p5a1Xj9VeNOI2Z44_gFw
"""

!pip install py7zr

import py7zr

archive_path = '/content/Churn-Prediction-Credit-Card-main.7z'
extract_path = '/content/'

with py7zr.SevenZipFile(archive_path, mode='r') as z:
    z.extractall(path=extract_path)

!pip install lightgbm xgboost catboost optuna scikit-learn pandas numpy matplotlib seaborn

print("üîß FIXED Model Training - No Target Leakage")
print("üéØ Implementing proper ML validation practices")
print("=" * 60)

import pandas as pd
import numpy as np
import pickle
import json
import os
import sys
from datetime import datetime
import time
import warnings
warnings.filterwarnings('ignore')

# Set working directory for Colab (uncomment for Colab)
os.chdir('/content/Churn-Prediction-Credit-Card-main/notebooks')
print(f"üìÇ Working directory: {os.getcwd()}")

# Add project root to path
sys.path.append('../')

# Core ML libraries
from sklearn.model_selection import (
    train_test_split, StratifiedKFold, cross_val_score,
    validation_curve, GridSearchCV, cross_validate
)
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, classification_report, confusion_matrix,
    roc_curve, precision_recall_curve
)

# Advanced ML libraries
try:
    import lightgbm as lgb
    print("‚úÖ LightGBM available")
except ImportError:
    print("‚ö†Ô∏è LightGBM not available")
    lgb = None

try:
    import xgboost as xgb
    print("‚úÖ XGBoost available")
except ImportError:
    print("‚ö†Ô∏è XGBoost not available")
    xgb = None

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Custom components
from src.exception import CustomException
from src.logger import logging

print(f"‚è∞ Environment loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("‚úÖ Ready to load CLEAN data without target leakage!\n")

print("üßπ Loading and Cleaning Data (Removing Target Leakage)...")
print("=" * 55)

def load_clean_data():
    """Load original data and remove target leakage columns."""
    try:
        # Load original raw data
        print("üìÇ Loading original dataset...")
        data_path = '../input/BankChurners.csv'
        if not os.path.exists(data_path):
            data_path = 'input/BankChurners.csv'

        df = pd.read_csv(data_path)
        print(f"   Original data shape: {df.shape}")
        print(f"   Columns: {list(df.columns)}")

        # CRITICAL: Identify and remove target leakage columns
        print("\nüö® Identifying target leakage columns...")
        leakage_patterns = ['naive_bayes', 'classifier', 'prediction', 'prob']
        leakage_columns = []

        for col in df.columns:
            col_lower = col.lower()
            if any(pattern in col_lower for pattern in leakage_patterns):
                leakage_columns.append(col)

        if leakage_columns:
            print(f"üî• FOUND TARGET LEAKAGE COLUMNS:")
            for col in leakage_columns:
                print(f"   ‚ùå {col}")

            print(f"\nüí• REMOVING {len(leakage_columns)} leakage columns...")
            df_clean = df.drop(columns=leakage_columns)
            print(f"   Clean data shape: {df_clean.shape}")
        else:
            print("‚úÖ No obvious target leakage columns found")
            df_clean = df.copy()

        # Prepare target variable
        print(f"\nüéØ Preparing target variable...")
        df_clean.columns = [x.lower().replace(' ', '_') for x in df_clean.columns]

        # Handle target column
        target_col = 'attrition_flag'
        if target_col in df_clean.columns:
            df_clean['churn_flag'] = df_clean[target_col].map({
                'Attrited Customer': 1,
                'Existing Customer': 0
            })
            df_clean = df_clean.drop(columns=[target_col])

        # Remove identifier columns
        id_columns = ['clientnum']
        existing_id_cols = [col for col in id_columns if col in df_clean.columns]
        if existing_id_cols:
            print(f"üóëÔ∏è Removing ID columns: {existing_id_cols}")
            df_clean = df_clean.drop(columns=existing_id_cols)

        # Final validation
        print(f"\n‚úÖ CLEAN DATASET READY:")
        print(f"   Shape: {df_clean.shape}")
        print(f"   Features: {df_clean.shape[1] - 1}")  # -1 for target
        print(f"   Target distribution:")
        print(f"     No Churn (0): {(df_clean['churn_flag'] == 0).sum()}")
        print(f"     Churn (1): {(df_clean['churn_flag'] == 1).sum()}")

        return df_clean

    except Exception as e:
        raise CustomException(e, sys)

# Load clean data
df_clean = load_clean_data()

print("\nüîÄ Creating Proper Train-Validation-Test Split...")
print("=" * 50)

def create_proper_splits(df, test_size=0.2, val_size=0.2, random_state=42):
    """Create proper train/validation/test splits to prevent data leakage."""

    try:
        # Separate features and target
        X = df.drop('churn_flag', axis=1)
        y = df['churn_flag']

        print(f"üìä Original data: {X.shape[0]} samples, {X.shape[1]} features")

        # First split: separate test set (20%)
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y,
            test_size=test_size,
            random_state=random_state,
            stratify=y
        )

        # Second split: separate train and validation from remaining 80%
        val_size_adjusted = val_size / (1 - test_size)  # Adjust for remaining data
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp,
            test_size=val_size_adjusted,
            random_state=random_state,
            stratify=y_temp
        )

        print(f"üéØ Data splits created:")
        print(f"   Training:   {X_train.shape[0]} samples ({len(X_train)/len(X)*100:.1f}%)")
        print(f"   Validation: {X_val.shape[0]} samples ({len(X_val)/len(X)*100:.1f}%)")
        print(f"   Test:       {X_test.shape[0]} samples ({len(X_test)/len(X)*100:.1f}%)")

        # Verify target distribution
        print(f"\nüìà Target distribution:")
        for split_name, y_split in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:
            churn_rate = (y_split == 1).mean()
            print(f"   {split_name}: {churn_rate:.3f} churn rate")

        # Check for data leakage (should be no overlap)
        train_index = set(X_train.index)
        val_index = set(X_val.index)
        test_index = set(X_test.index)

        assert len(train_index & val_index) == 0, "LEAKAGE: Train-Val overlap"
        assert len(train_index & test_index) == 0, "LEAKAGE: Train-Test overlap"
        assert len(val_index & test_index) == 0, "LEAKAGE: Val-Test overlap"

        print("‚úÖ No data leakage between splits confirmed")

        return X_train, X_val, X_test, y_train, y_val, y_test

    except Exception as e:
        raise CustomException(e, sys)

# Create splits
X_train, X_val, X_test, y_train, y_val, y_test = create_proper_splits(df_clean)

print("\nüîß Feature Engineering (Training Data Only)...")
print("=" * 45)

def safe_feature_engineering(X_train, X_val, X_test):
    """Apply feature engineering without target leakage."""

    try:
        from sklearn.preprocessing import StandardScaler, LabelEncoder

        # Identify categorical and numerical columns
        categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()
        numerical_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()

        print(f"üìä Feature types identified:")
        print(f"   Categorical: {len(categorical_cols)} columns")
        print(f"   Numerical: {len(numerical_cols)} columns")

        # STEP 1: Handle categorical variables (fit only on training data)
        print(f"\nüî§ Encoding categorical variables...")

        # Create copies to avoid modifying originals
        X_train_processed = X_train.copy()
        X_val_processed = X_val.copy()
        X_test_processed = X_test.copy()

        label_encoders = {}

        for col in categorical_cols:
            print(f"   Encoding: {col}")
            le = LabelEncoder()

            # Fit only on training data
            X_train_processed[col] = le.fit_transform(X_train_processed[col].astype(str))

            # Transform validation and test (may have unseen categories)
            try:
                X_val_processed[col] = le.transform(X_val_processed[col].astype(str))
                X_test_processed[col] = le.transform(X_test_processed[col].astype(str))
            except ValueError as e:
                # Handle unseen categories by assigning them to most frequent class
                print(f"     Warning: Unseen categories in {col}, using most frequent class")
                most_frequent = X_train[col].mode()[0] if len(X_train[col].mode()) > 0 else 'Unknown'
                most_frequent_encoded = le.transform([most_frequent])[0]

                # Replace unseen categories
                val_mask = ~X_val_processed[col].astype(str).isin(le.classes_)
                test_mask = ~X_test_processed[col].astype(str).isin(le.classes_)

                X_val_processed.loc[val_mask, col] = most_frequent_encoded
                X_test_processed.loc[test_mask, col] = most_frequent_encoded

                # Now apply transform
                X_val_processed[col] = le.transform(X_val_processed[col].astype(str))
                X_test_processed[col] = le.transform(X_test_processed[col].astype(str))

            label_encoders[col] = le

        # STEP 2: Scale numerical features (fit only on training data)
        print(f"\nüìè Scaling numerical features...")

        if numerical_cols:
            scaler = StandardScaler()

            # Fit only on training data
            X_train_processed[numerical_cols] = scaler.fit_transform(X_train_processed[numerical_cols])

            # Transform validation and test
            X_val_processed[numerical_cols] = scaler.transform(X_val_processed[numerical_cols])
            X_test_processed[numerical_cols] = scaler.transform(X_test_processed[numerical_cols])

            print(f"   Scaled {len(numerical_cols)} numerical features")

        print(f"\n‚úÖ Feature engineering complete:")
        print(f"   Train shape: {X_train_processed.shape}")
        print(f"   Val shape: {X_val_processed.shape}")
        print(f"   Test shape: {X_test_processed.shape}")

        return X_train_processed, X_val_processed, X_test_processed, label_encoders

    except Exception as e:
        raise CustomException(e, sys)

# Apply feature engineering
X_train_processed, X_val_processed, X_test_processed, label_encoders = safe_feature_engineering(
    X_train, X_val, X_test
)

print("\nü§ñ Model Training with Cross-Validation...")
print("=" * 45)

def train_models_with_validation(X_train, y_train, X_val, y_val):
    """Train models with proper validation to prevent overfitting."""

    models = {}
    results = {}
    cv_results = {}

    # Use StratifiedKFold for cross-validation
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    print("üî• Training models with 5-fold cross-validation...")

    # 1. Logistic Regression with regularization
    print("\nüìä 1. Training Logistic Regression...")
    lr = LogisticRegression(
        random_state=42,
        max_iter=1000,
        C=1.0,  # L2 regularization
        class_weight='balanced'  # Handle class imbalance
    )

    # Cross-validation on training set
    cv_scores = cross_val_score(lr, X_train, y_train, cv=cv, scoring='roc_auc')
    cv_results['Logistic Regression'] = {
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'cv_scores': cv_scores.tolist()
    }

    # Fit on training and evaluate on validation
    lr.fit(X_train, y_train)
    lr_val_pred = lr.predict(X_val)
    lr_val_proba = lr.predict_proba(X_val)[:, 1]

    models['Logistic Regression'] = lr
    results['Logistic Regression'] = {
        'val_accuracy': accuracy_score(y_val, lr_val_pred),
        'val_precision': precision_score(y_val, lr_val_pred),
        'val_recall': recall_score(y_val, lr_val_pred),
        'val_f1': f1_score(y_val, lr_val_pred),
        'val_auc': roc_auc_score(y_val, lr_val_proba)
    }

    # 2. Random Forest with regularization
    print("üå≥ 2. Training Random Forest...")
    rf = RandomForestClassifier(
        n_estimators=100,  # Reduced to prevent overfitting
        max_depth=10,      # Limit depth
        min_samples_split=20,  # Require more samples to split
        min_samples_leaf=10,   # Require more samples in leaf
        random_state=42,
        class_weight='balanced',
        n_jobs=-1
    )

    # Cross-validation
    cv_scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='roc_auc')
    cv_results['Random Forest'] = {
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'cv_scores': cv_scores.tolist()
    }

    # Validation
    rf.fit(X_train, y_train)
    rf_val_pred = rf.predict(X_val)
    rf_val_proba = rf.predict_proba(X_val)[:, 1]

    models['Random Forest'] = rf
    results['Random Forest'] = {
        'val_accuracy': accuracy_score(y_val, rf_val_pred),
        'val_precision': precision_score(y_val, rf_val_pred),
        'val_recall': recall_score(y_val, rf_val_pred),
        'val_f1': f1_score(y_val, rf_val_pred),
        'val_auc': roc_auc_score(y_val, rf_val_proba)
    }

    # 3. Gradient Boosting with regularization
    print("‚ö° 3. Training Gradient Boosting...")
    gb = GradientBoostingClassifier(
        n_estimators=100,
        learning_rate=0.1,   # Moderate learning rate
        max_depth=5,         # Shallow trees
        min_samples_split=20,
        min_samples_leaf=10,
        subsample=0.8,       # Stochastic gradient boosting
        random_state=42
    )

    # Cross-validation
    cv_scores = cross_val_score(gb, X_train, y_train, cv=cv, scoring='roc_auc')
    cv_results['Gradient Boosting'] = {
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'cv_scores': cv_scores.tolist()
    }

    # Validation
    gb.fit(X_train, y_train)
    gb_val_pred = gb.predict(X_val)
    gb_val_proba = gb.predict_proba(X_val)[:, 1]

    models['Gradient Boosting'] = gb
    results['Gradient Boosting'] = {
        'val_accuracy': accuracy_score(y_val, gb_val_pred),
        'val_precision': precision_score(y_val, gb_val_pred),
        'val_recall': recall_score(y_val, gb_val_pred),
        'val_f1': f1_score(y_val, gb_val_pred),
        'val_auc': roc_auc_score(y_val, gb_val_proba)
    }

    # 4. LightGBM (if available)
    if lgb is not None:
        print("üí° 4. Training LightGBM...")
        lgbm = lgb.LGBMClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            min_child_samples=20,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,      # L1 regularization
            reg_lambda=0.1,     # L2 regularization
            class_weight='balanced',
            random_state=42,
            verbose=-1
        )

        # Cross-validation
        cv_scores = cross_val_score(lgbm, X_train, y_train, cv=cv, scoring='roc_auc')
        cv_results['LightGBM'] = {
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'cv_scores': cv_scores.tolist()
        }

        # Validation
        lgbm.fit(X_train, y_train)
        lgbm_val_pred = lgbm.predict(X_val)
        lgbm_val_proba = lgbm.predict_proba(X_val)[:, 1]

        models['LightGBM'] = lgbm
        results['LightGBM'] = {
            'val_accuracy': accuracy_score(y_val, lgbm_val_pred),
            'val_precision': precision_score(y_val, lgbm_val_pred),
            'val_recall': recall_score(y_val, lgbm_val_pred),
            'val_f1': f1_score(y_val, lgbm_val_pred),
            'val_auc': roc_auc_score(y_val, lgbm_val_proba)
        }

    return models, results, cv_results

# Train models
start_time = time.time()
models, val_results, cv_results = train_models_with_validation(
    X_train_processed, y_train, X_val_processed, y_val
)
training_time = time.time() - start_time

print(f"\n‚è±Ô∏è Training completed in {training_time:.2f} seconds")

print("\nüìä Model Performance Analysis (Cross-Validation + Validation)...")
print("=" * 65)

def analyze_realistic_performance(val_results, cv_results):
    """Analyze model performance showing realistic results."""

    print("üéØ CROSS-VALIDATION RESULTS (5-Fold):")
    print("=" * 40)

    for model_name, cv_res in cv_results.items():
        print(f"\n{model_name}:")
        print(f"   CV AUC: {cv_res['cv_mean']:.4f} ¬± {cv_res['cv_std']:.4f}")
        print(f"   CV Range: [{cv_res['cv_mean'] - cv_res['cv_std']:.4f}, {cv_res['cv_mean'] + cv_res['cv_std']:.4f}]")

    print(f"\n\nüéØ VALIDATION SET RESULTS:")
    print("=" * 30)

    # Create performance DataFrame
    perf_df = pd.DataFrame(val_results).T
    perf_df = perf_df.round(4)

    # Sort by validation AUC
    perf_df_sorted = perf_df.sort_values('val_auc', ascending=False)

    for i, (model, metrics) in enumerate(perf_df_sorted.iterrows(), 1):
        print(f"\n{i}. {model}")
        print(f"   Val AUC: {metrics['val_auc']:.4f}")
        print(f"   Val Accuracy: {metrics['val_accuracy']:.4f}")
        print(f"   Val Precision: {metrics['val_precision']:.4f}")
        print(f"   Val Recall: {metrics['val_recall']:.4f}")
        print(f"   Val F1: {metrics['val_f1']:.4f}")

    # Check for overfitting
    print(f"\nüîç OVERFITTING ANALYSIS:")
    print("=" * 25)

    for model_name in cv_results.keys():
        if model_name in val_results:
            cv_auc = cv_results[model_name]['cv_mean']
            val_auc = val_results[model_name]['val_auc']
            difference = cv_auc - val_auc

            print(f"\n{model_name}:")
            print(f"   CV AUC: {cv_auc:.4f}")
            print(f"   Val AUC: {val_auc:.4f}")
            print(f"   Difference: {difference:.4f}", end="")

            if abs(difference) < 0.02:
                print(" ‚úÖ Good generalization")
            elif difference > 0.05:
                print(" ‚ö†Ô∏è Possible overfitting")
            else:
                print(" üîÑ Acceptable")

    # Find best model
    best_model_name = perf_df_sorted.index[0]
    best_metrics = perf_df_sorted.iloc[0]

    return perf_df_sorted, best_model_name, best_metrics

# Analyze performance
perf_df, best_model_name, best_metrics = analyze_realistic_performance(val_results, cv_results)

print(f"\nüèÜ Final Test Set Evaluation (Best Model: {best_model_name})...")
print("=" * 60)

def final_test_evaluation(best_model, X_test, y_test, model_name):
    """Final unbiased evaluation on test set."""

    try:
        print(f"üéØ Evaluating {model_name} on unseen test data...")

        # Get predictions
        test_pred = best_model.predict(X_test)
        test_proba = best_model.predict_proba(X_test)[:, 1]

        # Calculate metrics
        test_results = {
            'accuracy': accuracy_score(y_test, test_pred),
            'precision': precision_score(y_test, test_pred),
            'recall': recall_score(y_test, test_pred),
            'f1': f1_score(y_test, test_pred),
            'auc': roc_auc_score(y_test, test_proba)
        }

        print(f"\nüéâ FINAL TEST RESULTS:")
        print(f"=" * 25)
        print(f"Model: {model_name}")
        print(f"Test AUC: {test_results['auc']:.4f}")
        print(f"Test Accuracy: {test_results['accuracy']:.4f}")
        print(f"Test Precision: {test_results['precision']:.4f}")
        print(f"Test Recall: {test_results['recall']:.4f}")
        print(f"Test F1-Score: {test_results['f1']:.4f}")

        # Confusion matrix
        cm = confusion_matrix(y_test, test_pred)
        print(f"\nüìä Confusion Matrix:")
        print(f"True Negatives: {cm[0,0]}")
        print(f"False Positives: {cm[0,1]}")
        print(f"False Negatives: {cm[1,0]}")
        print(f"True Positives: {cm[1,1]}")

        return test_results

    except Exception as e:
        raise CustomException(e, sys)

# Final evaluation
best_model = models[best_model_name]
test_results = final_test_evaluation(
    best_model, X_test_processed, y_test, best_model_name
)

print(f"\nüíæ Saving Clean Results...")
print("=" * 30)

def save_clean_results():
    """Save all results without target leakage."""

    try:
        # Set the base results directory
        results_dir = '/content/results'

        # Ensure the results directory exists
        os.makedirs(results_dir, exist_ok=True)
        print(f"‚úÖ Ensured results directory exists: {results_dir}")

        # Save best model
        model_path = os.path.join(results_dir, 'clean_best_model.pkl')
        with open(model_path, 'wb') as f:
            pickle.dump(best_model, f)
        print(f"‚úÖ Saved: {model_path}")

        # Save comprehensive results
        clean_results = {
            'experiment_info': {
                'timestamp': datetime.now().isoformat(),
                'target_leakage_removed': True,
                'proper_validation': True,
                'best_model': best_model_name
            },
            'data_info': {
                'original_features': len(df_clean.columns) - 1,
                'samples': {
                    'train': len(X_train),
                    'validation': len(X_val),
                    'test': len(X_test)
                },
                'target_distribution': {
                    'no_churn': int((y_train == 0).sum()),
                    'churn': int((y_train == 1).sum())
                }
            },
            'cross_validation_results': cv_results,
            'validation_results': {k: {k2: float(v2) for k2, v2 in v.items()} for k, v in val_results.items()},
            'final_test_results': {k: float(v) for k, v in test_results.items()},
            'model_comparison': {
                'best_model': best_model_name,
                'performance_ranking': perf_df.to_dict('index')
            }
        }

        results_path = os.path.join(results_dir, 'clean_training_results.json')
        with open(results_path, 'w') as f:
            json.dump(clean_results, f, indent=2)
        print(f"‚úÖ Saved: {results_path}")

        return clean_results

    except Exception as e:
        raise CustomException(e, sys)

# Save results
clean_results = save_clean_results()

# Final Summary
print(f"\nüéâ CLEAN MODEL TRAINING COMPLETED!")
print("=" * 40)
print(f"‚úÖ Target leakage eliminated")
print(f"‚úÖ Proper train/val/test splits")
print(f"‚úÖ Cross-validation implemented")
print(f"‚úÖ Regularization applied")
print(f"‚úÖ Realistic performance metrics")
print(f"\nüèÜ Best Model: {best_model_name}")
print(f"üéØ Test AUC: {test_results['auc']:.4f} (Realistic!)")
print(f"üìä Test Accuracy: {test_results['accuracy']:.4f}")
print(f"\nüìÅ Clean artifacts saved:")
print(f"   ‚úÖ clean_best_model.pkl")
print(f"   ‚úÖ clean_training_results.json")
print(f"\nüöÄ Ready for production deployment!")